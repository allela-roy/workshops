{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4268eb5a-c4c2-4d83-914d-396333bce135",
   "metadata": {},
   "source": [
    "### Image Similarity\n",
    "\n",
    "This notebook will lead you through:\n",
    "\n",
    "1. Downloading the image embedding model `vit_base_patch16_224.augreg2_in21k_ft_in1k` from HuggingFace using the built-in interface in Python.\n",
    "2. Input digitized images to the AI model and retrieve embeddings for them\n",
    "3. Measure the cosines between embeddings to see that they match intuitions about the similarity of the content of those pictures.\n",
    "\n",
    "If you are running this notebook in an environment that may not have all the prerequisites installed, run the line below first. It will install the necessary libraries if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874a58e-2b1d-45be-ab81-4951b7d2d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy torch timm sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310bfa4-6681-404c-baf4-842dcaa61e39",
   "metadata": {},
   "source": [
    "First, we import the Python PIL library, which contains functions for manipulating image files, and the `requests` library, which we will use to fetch images from the internet by their URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb986e-274c-479b-abc4-21aa5a895c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdcf62d-646e-422a-96a2-f36ac4f6f9f3",
   "metadata": {},
   "source": [
    "Next, we are going to fetch a small collection of images, two of apples, one of an orange, and one of a dog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1305657-42d9-4da5-9540-dd0477b097db",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_apple_1 = Image.open(requests.get('https://raw.githubusercontent.com/jina-ai/workshops/gymhouse/gymhouse/images/apple.jpg', stream=True).raw)\n",
    "image_apple_2 = Image.open(requests.get('https://raw.githubusercontent.com/jina-ai/workshops/gymhouse/gymhouse/images/apple2.png', stream=True).raw)\n",
    "image_orange = Image.open(requests.get('https://raw.githubusercontent.com/jina-ai/workshops/gymhouse/gymhouse/images/orange.png', stream=True).raw)\n",
    "image_dog = Image.open(requests.get('https://raw.githubusercontent.com/jina-ai/workshops/gymhouse/gymhouse/images/dog.png', stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80bfed-8130-4e0a-ad20-f0c90d3a2ef3",
   "metadata": {},
   "source": [
    "You can inspect the individual images just by entering them in a notebook input field and pressing enter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824cde2-4b64-4bc8-b00a-7cd287aec34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_apple_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b5d0e-c75e-49da-9324-7b8c2e997e7f",
   "metadata": {},
   "source": [
    "Now we will import the `timm` library to load and modify the model and then download the `vit_base_patch16_224.augreg2_in21k_ft_in1k` model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0d936-68a6-4192-a862-48de63dac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model =  timm.create_model('vit_base_patch16_224.augreg2_in21k_ft_in1k', pretrained=True, num_classes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85b852-ff16-4ee9-b92b-203cc3a42878",
   "metadata": {},
   "source": [
    "Loading the model this way gets rid of the layer that classifies the output into the 1,000 categories it was trained for and makes the last layer the embeddings layer.\n",
    "\n",
    "Images also require some pre-processing before they can become input to the model. We will use the model’s configuration file to create a function `transformer()` that converts Python’s internal image format into an appropriate input vector for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419919f-9bac-449a-8734-b1c9f6226d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "tf = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "def transformer(input):\n",
    "    return tf(input).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13860ff4-96e6-4630-a747-dc4219c6703e",
   "metadata": {},
   "source": [
    "You can see it in action by giving `transformer()` an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125de68-32d8-4b04-b2da-b10da4273e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer(image_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98591b9-4716-42ec-bc0b-4fbd88b9bab6",
   "metadata": {},
   "source": [
    "To get an embedding, we just pass this to the model:\n",
    "\n",
    "(Don’t worry about the `.squeeze(0).detach().numpy()` part. This is just a way to convert the internal data format the AI model uses to one more convenient for us to use. Some AI software does this automatically, others do not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac86462-a687-4c2f-bf01-a5606d231734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(transformer(image_dog)).squeeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14775e40-ba9c-4094-a2b0-bba2cd4d70c7",
   "metadata": {},
   "source": [
    "Each embedding from this model has 576 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac402233-632d-4db8-9561-629444430676",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dog = model(transformer(image_dog)).squeeze(0).detach().numpy()\n",
    "len(embedding_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd051c-5b5f-483f-95b5-c993caa5b9b6",
   "metadata": {},
   "source": [
    "Let's get embeddings for the remaining three images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5c348-ca9c-4c99-8f7c-d185b1e52046",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_apple1 = model(transformer(image_apple_1)).squeeze(0).detach().numpy()\n",
    "embedding_apple2 = model(transformer(image_apple_2)).squeeze(0).detach().numpy()\n",
    "embedding_orange = model(transformer(image_orange)).squeeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de49166-091f-44db-a05d-11739ff96e41",
   "metadata": {},
   "source": [
    "Define the cosine function over pairs of vectors, so we can compare embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561a07c-e647-4b18-b867-e49d404f8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(a, b):\n",
    "    return dot(a,b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0850414-5c18-4c14-b545-e0b8994b5f60",
   "metadata": {},
   "source": [
    "The two apple images have a pretty high cosine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5f9ae-2f7e-458e-b568-102a3a79f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embedding_apple1, embedding_apple2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342c8cc-748d-4566-9bfd-7e8b8f0e0145",
   "metadata": {},
   "source": [
    "At least when compared to the orange image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9ab4f-c51b-461c-bcb3-a1004bc1f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Apple 1 to Orange: {cosine(embedding_apple1, embedding_orange)}\")\n",
    "print(f\"Apple 2 to Orange: {cosine(embedding_apple2, embedding_orange)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bb800-1452-4499-afcc-1cd1fd7b1f27",
   "metadata": {},
   "source": [
    "And all three fruit pictures are far from the dog picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9c937-6b61-44ef-9290-4331a4ee0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Apple 1 to Dog: {cosine(embedding_apple1, embedding_dog)}\")\n",
    "print(f\"Apple 2 to Dog: {cosine(embedding_apple2, embedding_dog)}\")\n",
    "print(f\"Orange to Dog: {cosine(embedding_orange, embedding_dog)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45649e6b-dd24-4c1e-b150-a06fe9a3570f",
   "metadata": {},
   "source": [
    "You can try this with your own images to see if it matches your intuitions.\n",
    "\n",
    "To load an image from a URI, you can use:\n",
    "\n",
    "```python\n",
    "your_image = Image.open(requests.get('<your image URI>', stream=True).raw)\n",
    "```\n",
    "\n",
    "To load one from a file, just use\n",
    "\n",
    "```python\n",
    "your_image = Image.open('<your image file path>')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f953fc-d0dc-405d-9838-4a6c9f36e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
