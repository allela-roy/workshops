{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM3bsA_cgKcs",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "![](https://miro.medium.com/max/700/1*pJOoQOlHCses8zvefz3YFg.png)\n",
    "\n",
    "# Build a Stack Overflow search engine with Python and ML\n",
    "\n",
    "This tutorial helps you build an ML-powered search engine for Stack Overflow data while introducing [DocArray](https://docarray.jina.ai?utm_source=stack-overflow-notebook) and [Jina](https://docs.jina.ai). A user can input a text query and then retrieve questions and answers where the question title is similar to the query.\n",
    "\n",
    "Throughout the notebook we'll have some sections called ⚙️ **Tinker time**, which explain some common changes you may want to make to the code.\n",
    "\n",
    "## Meet our ingredients\n",
    "\n",
    "### **[DocArray](https://docarray.jina.ai?utm_source=stack-overflow-notebook)**\n",
    "\n",
    "DocArray is a library for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the multi-modal data with a Pythonic API. ([star the repo]())\n",
    "\n",
    "### **[Jina](https://docs.jina.ai)**\n",
    " \n",
    " Jina is a framework that empowers anyone to build cross-modal and multi-modal[*] applications on the cloud. It uplifts a PoC into a production-ready service. Jina handles the infrastructure complexity, making advanced solution engineering and cloud-native technologies accessible to every developer. ([star the repo]())\n",
    "\n",
    "### **[Jina Hub](https://hub.jina.ai)**\n",
    "\n",
    "Download pre-built building blocks for neural search.\n",
    "\n",
    "\n",
    "### **[Stack Overflow R dataset](https://www.kaggle.com/datasets/stackoverflow/rquestions)**\n",
    "\n",
    "Why not use the [Python dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions)? When I tried reading in the CSV I got a few encoding errors and it frankly wasn't worth the headache.\n",
    "\n",
    "## Why this tech stack?\n",
    "\n",
    "I'm using the Jina ecosystem because:\n",
    "\n",
    "- I don't have to manually integrate a lot of stuff like encoders and indexers. I can just pull them with one line of code.\n",
    "- Switching out one encoder (e.g. [spaCy](https://hub.jina.ai/executor/u7h7cuh2)) for another (e.g. [Transformers](https://hub.jina.ai/executor/u9pqs8eb)) is a matter of just changing a couple of lines. This is great for tinkering and seeing what works best.\n",
    "- I can run compute-heavy tasks (e.g. encoding) on a cloud [sandbox](https://docs.jina.ai/how-to/sandbox?utm_source=stack-overflow-notebook) really easily, freeing up my own resources. (This applies mostly to running your notebook locally or adapting it for production)\n",
    "- If I were to productionize this example, I could easily increase speed and reliability via [sharding or replicas](https://docs.jina.ai/how-to/scale-out?utm_source=stack-overflow-notebook). And running on [Kubernetes](https://docs.jina.ai/how-to/kubernetes/) is a breeze.\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by installing DocArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vi-MaDzrcoyY",
    "outputId": "b66c54ea-5726-41a8-9468-6c63fd4cfd6e"
   },
   "outputs": [],
   "source": [
    "!pip install -q docarray==0.13.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SepMlwWLvpYk"
   },
   "source": [
    "...and then importing [DocumentArray](ttps://docarray.jina.ai/fundamentals/documentarray?utm_source=stack-overflow-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL_webpicl4T"
   },
   "outputs": [],
   "source": [
    "from docarray import DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nixFjQgofvQV"
   },
   "source": [
    "## Downloading our Data\n",
    "\n",
    "Unfortunately Colab notebooks don't save state, so we can't store our data alongside our notebook. So how can we convert our CSV from the dataset?\n",
    "\n",
    "We could remedy this in two ways:\n",
    "\n",
    "1. Download the CSV and [import directly](https://docarray.jina.ai/datatypes/tabular?utm_source=stack-overflow-notebook) into a [DocumentArray](https://docarray.jina.ai/fundamentals/documentarray/) with `docs = DocumentArray.from_csv(\"Questions.csv\")`. This is tricky since it's stored on Kaggle and I don't really want to share my Kaggle key publicly. Or...\n",
    "\n",
    "2. Here's one I made earlier! In one command we can [pull in a pre-existing DocumentArray from the cloud](https://docarray.jina.ai/fundamentals/documentarray/serialization/?highlight=pull#from-to-cloud). We'll just use the first 1,000 questions in the dataset since this is a demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9mIyBlM0foZz",
    "outputId": "42813ce8-134f-45f0-abd5-02677c4ad433"
   },
   "outputs": [],
   "source": [
    "docs = DocumentArray.pull(name=\"stack_overflow_r_q\")[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### ⚙️ Tinker time\n",
    "\n",
    "All data that goes into our pipeline needs to be in the form of a [Document]() or [DocumentArray](). They can store any kind of data, so whether we were making an image search engine, text search engine, 3D mesh search engine or whatever, all data would be stored in this data type.\n",
    "\n",
    "There are several quick ways to create a DocumentArray:\n",
    "\n",
    "- [From CSV](https://docarray.jina.ai/datatypes/tabular?utm_source=stack-overflow-notebook): `DocumentArray.from_csv('toy.csv', field_resolver={'Title': 'text'})` - every row of a CSV becomes a Document, with the `Title` field as the primary data (which will be processed) and other fields as metadata tags.\n",
    "- [From a folder](https://docarray.jina.ai/fundamentals/documentarray/construct/#construct-from-local-files): `DocumentArray.from_files(\"data/**/*.jpg\", recursive=True)` - every file in the glob pattern is stored as a Document in the DocumentArray\n",
    "- [From JSON](https://docarray.jina.ai/fundamentals/documentarray/serialization/#from-to-json): `DocumentArray.from_json(\"foo.json\")` - every record in the JSON file becomes a Document\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRcz5nzzwtXj"
   },
   "source": [
    "Let's see what's we've got. As we can see, 1,000 [Documents](https://docarray.jina.ai/fundamentals/document?utm_source=stack-overflow-notebook), each with:\n",
    "- The title of the question in `doc.text` - this is what will be encoded later in our [Flow](https://docs.jina.ai/fundamentals/flow?utm_source=stack-overflow-notebook).\n",
    "- Tags - i.e. metadata, containing a `dict` of all the other fields associated with that question title.\n",
    "- ID - a unique identifier for each Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "PwBeUcDdgBSe",
    "outputId": "daaff516-2993-4bf7-f27a-18014baaef42"
   },
   "outputs": [],
   "source": [
    "docs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gTBByke_thC"
   },
   "source": [
    "Let's take a closer look at a single Document so we can get an idea of the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqALx3rr_sij",
    "outputId": "485e5fd8-a701-4f9b-ac2c-805cd2b18a96"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint # pretty-print makes it easier for humans to read dicts\n",
    "\n",
    "print(docs[0].text)\n",
    "pprint(docs[0].tags)\n",
    "pprint(docs[0].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7hxXX6uhUN7"
   },
   "source": [
    "## Setting up our Flow\n",
    "\n",
    "To build a search engine we need to pass our Documents into a [Flow](https://docs.jina.ai/fundamentals/flow?utm_source=stack-overflow-notebook). This is what will create embeddings and store our Documents in an index for fast look-up later.\n",
    "\n",
    "We'll use the [Jina](https://docs.jina.ai?utm_source=stack-overflow-notebook) package to build and orchestrate our Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um_QVpqSgpC4",
    "outputId": "fd624a24-a2a7-4806-f575-f1bfbf910528"
   },
   "outputs": [],
   "source": [
    "!pip install -q jina==3.6.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znTGU0IXKpbd"
   },
   "source": [
    "Creating a Flow is a matter of chaining together building blocks (a.k.a [Executors](https://docs.jina.ai/fundamentals/executor?utm_source=stack-overflow-notebook)). In our case we won't [write these manually](https://docs.jina.ai/fundamentals/executor/executor-api/), but rather we'll either download them from [Jina Hub](https://hub.jina.ai) or run them in a [sandbox in the cloud](https://docs.jina.ai/how-to/sandbox/?highlight=sandbox). This will save us some time and effort.\n",
    "\n",
    "Let's start by creating an empty Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzdWZJyGiHrt"
   },
   "outputs": [],
   "source": [
    "from jina import Flow\n",
    "\n",
    "flow = Flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXc-gNEOLNy7"
   },
   "source": [
    "Now we'll add our **encoder**. This will encode the text from each Document into vector embeddings. We'll need these for matching similar text later on.\n",
    "\n",
    "In our case we'll use [SpacyTextEncoder](https://hub.jina.ai/executor/u7h7cuh2) with the medium language model, though you could swap it out easily for other encoders like [Transformers](https://hub.jina.ai/executor/u9pqs8eb). I typically use spaCy because it's been much faster than alternatives (at least in my experience).\n",
    "\n",
    "We'll use the [medium English model](https://spacy.io/models/en#en_core_web_md) (`en_core_web_md`) since I find that's a good balance between accuracy and performance. \n",
    "\n",
    "We'll run it in a sandbox in the cloud. This way, even if you run this notebook on your own machine, you won't be using your own compute for intensive tasks like generating encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mg27PBtLkhU"
   },
   "outputs": [],
   "source": [
    "flow = flow.add(\n",
    "    name=\"encoder\",\n",
    "    uses=\"jinahub+sandbox://SpacyTextEncoder/v0.4\",\n",
    "    uses_with={\"model_name\": \"en_core_web_md\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### ⚙️ **Tinker time**\n",
    "\n",
    "If you want to tinker, you can swap out the Executor below by changing the `uses` value and `uses_with` parameters. So if we wanted to use [Transformers](https://hub.jina.ai/executor/u9pqs8eb) to encode our Documents, we could change our code to:\n",
    "\n",
    "```python\n",
    "flow = flow.add(\n",
    "    name=\"encoder\",\n",
    "    uses=\"jinahub+sandbox://TransformerTorchEncoder\",\n",
    "    uses_with={\"pretrained_model_name_or_path\": \"bert-base-uncased\"}\n",
    ")\n",
    "```\n",
    "\n",
    "If you stick with spaCy, you could also change `model_name` to:\n",
    "\n",
    "- [`en_core_web_sm`](https://spacy.io/models/en#en_core_web_sm) - Smaller model. Likely less accurate, but faster performance.\n",
    "- [`en_core_web_lg`](https://spacy.io/models/en#en_core_web_lg) - Larger model. Potentially more accurate, but slower performance.\n",
    "\n",
    "spaCy also offers [models in other languages](https://spacy.io/models).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GThDr4UGL_Ix"
   },
   "source": [
    "Next we'll add our indexer. This takes the vector embeddings and metadata and stores them in a database for fast lookup when a user is searching.\n",
    "\n",
    "We'll use [AnnLiteIndexer](https://hub.jina.ai/executor/7yypg8qk), which will store our data in a SQLite database. For production use, other indexers like [HNSWPostgresIndexer](https://hub.jina.ai/executor/dvp0845a) may be more suitable, but for a simple notebook this is a good fit. AnnLite also has the benefit that we can filter our search by tags. We won't do that in this notebook, but its a nice future option.\n",
    "\n",
    "In this case we won't run AnnLite in a sandbox, since we want our indexed data stored in the same place as our notebook, not on some cloud machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLhN_KQTM6uF"
   },
   "outputs": [],
   "source": [
    "flow = flow.add(\n",
    "    name=\"indexer\",\n",
    "    uses=\"jinahub://AnnLiteIndexer/0.3.0\",\n",
    "    uses_with={\"dim\": 300},  # we're using a 300 dimension model\n",
    "    # uses_metas={\"workspace\": \"workspace\"},  # this is where we'll store our data on disk\n",
    "    install_requirements=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### ⚙️ Tinker time\n",
    "\n",
    "There are lots of ways to run an Executor:\n",
    "\n",
    "- `uses=jinahub://foo` - downloads the Executor source from [Hub](https://hub.jina.ai) to your local machine and runs it there (don't forget `install_requirements`!)\n",
    "- `uses=jinahub+docker://foo` - downloads and runs the Executor's Docker image on your machine\n",
    "- `uses=jinahub+sandbox://foo` - runs the Executor directly in Jina's cloud [sandbox](https://docs.jina.ai/how-to/sandbox/?highlight=sandbox), saving you compute\n",
    "- `uses=Foo` - run an Executor that you've [built yourself](https://docs.jina.ai/fundamentals/executor/executor-api/#)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqC2VQICxmJk"
   },
   "source": [
    "Let's preview our Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "sgtY4vvjsNA4",
    "outputId": "a2d79c14-848f-4b35-ad57-3a838e9a8b1c"
   },
   "outputs": [],
   "source": [
    "flow.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJY4O50mNLxw"
   },
   "source": [
    "## Indexing our data\n",
    "\n",
    "That's our Flow built. Now we can run it to start pushing our data through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186,
     "referenced_widgets": [
      "5b35499811e94bee8c2692361f4d9915",
      "c66e25413bb24fce843e44795e4cc25c"
     ]
    },
    "id": "nK9vR5sXj0dX",
    "outputId": "009be3c9-331f-4b1b-9b93-bdb77d7af027"
   },
   "outputs": [],
   "source": [
    "with flow:\n",
    "    docs = flow.index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bib7-4MTNk1Y"
   },
   "source": [
    "## Searching our data\n",
    "\n",
    "Now that we've built our index, it's time to do some searching!\n",
    "\n",
    "Everything we've worked with while indexing has been in the form of a [Document](https://docarray.jina.ai/fundamentals/document?utm_source=stack-overflow-notebook) (stored in a DocumentArray). So we'll need to create another Document for searching that index.\n",
    "\n",
    "Feel free to change `search_term` to your own query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186,
     "referenced_widgets": [
      "fea82d7b793a4beb987f541c61b39251",
      "2062610f348d40dbbdf140a27509b4a8"
     ]
    },
    "id": "eDxKsIXGk63b",
    "outputId": "0914c0da-1662-4e31-cc06-0dec3fbf0356"
   },
   "outputs": [],
   "source": [
    "from docarray import Document\n",
    "\n",
    "search_term = \"How do I create a matrix?\"\n",
    "query = Document(text=search_term)\n",
    "\n",
    "with flow:\n",
    "  results = flow.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUqBNxAAOBTc"
   },
   "source": [
    "Now to look at what matched our search term. `results` is also a DocumentArray (can you see the pattern?). We'll access its `matches` attribute and see what's stored inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cWUmof_mo7U",
    "outputId": "9087a9a2-23ca-4933-9dcd-c32f9da69a83"
   },
   "outputs": [],
   "source": [
    "matches = results[0].matches\n",
    "\n",
    "for match in matches:\n",
    "  print(match.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWwiDHzEOWAJ"
   },
   "source": [
    "## Getting answers to our questions\n",
    "\n",
    "So far, so good. We've got a list of matching questions. But how can we pair those with the relevant answers?\n",
    "\n",
    "First we'll need to download our answers. In this case we won't limit them to just 1,000 because:\n",
    "\n",
    "* Many questions have more than one answer.\n",
    "* The order may be different, so the first question in our dataset may have answer 1,234, 50,234 or 1,337 as its answer.\n",
    "\n",
    "Once again, we'll [pull from the cloud](https://docarray.jina.ai/fundamentals/documentarray/serialization/?highlight=pull#from-to-cloud):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "rCi0uO8mmvLH",
    "outputId": "0f36a188-926a-473a-bb96-8703a344e71f"
   },
   "outputs": [],
   "source": [
    "answers = DocumentArray.pull(name=\"stack_overflow_r_a\")\n",
    "answers.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W70jbzBXPYZh"
   },
   "source": [
    "Now we can use the [`find` method](https://docarray.jina.ai/fundamentals/documentarray/find?utm_source=stack-overflow-notebook) to dig out answers where the answer's `ParentId` tag matches the question's `Id` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2hWZJhkpp59",
    "outputId": "dbb68fec-98d3-4d6e-b0f3-325881569fc2"
   },
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "  print(match.text)\n",
    "  match_answers = answers.find({\"tags__ParentId\": {\"$eq\": match.tags[\"Id\"]}})\n",
    "  for answer in match_answers:\n",
    "    print(\"---\")\n",
    "    print(answer.text)\n",
    "  print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRtwzsWbQBrV"
   },
   "source": [
    "Voila! You can see:\n",
    "\n",
    "* Questions matching our search term\n",
    "* Answers to those questions\n",
    "\n",
    "Admittedly, the HTML formatting looks a bit janky, but if you were using this IRL you'd strip that out or properly display it. Since this is just a notebook I'll leave that as an exercise for you, dear reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDAfJmqIQXDv"
   },
   "source": [
    "## Putting it into production\n",
    "\n",
    "Colab notebooks have a number of restrictions that make cool stuff quite difficult. If we were building this outside of a notebook, we could:\n",
    "\n",
    "* Set up a [RESTful or gRPC gateway](https://docs.jina.ai/fundamentals/gateway?utm_source=stack-overflow-notebook) and keep the Flow open to requests using `flow.block()`\n",
    "* Use [sharding and replicas](https://docs.jina.ai/how-to/scale-out?utm_source=stack-overflow-notebook) to improve performance and reliability.\n",
    "* [Monitor our Flow with Grafana](https://docs.jina.ai/fundamentals/flow/monitoring-flow?utm_source=stack-overflow-notebook)\n",
    "* Better yet, host our Flow on [JCloud](https://docs.jina.ai/fundamentals/jcloud?utm_source=stack-overflow-notebook), so we don't have to use any of our own compute for encoding, indexing, hosting, etc (encoding is especially hungry on the hardware)\n",
    "* Finetune our results using [Finetuner](https://finetuner.jina.ai) to provide better matches\n",
    "* Use a more specialized model for dealing with technical/code queries (rather than just general purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "Want to dig more into the Jina ecosystem? Here are some resources:\n",
    "\n",
    "- [Developer portal](https://learn.jina.ai) - tutorials, courses, videos on using Jina\n",
    "- [Fashion search notebook](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb) - build an image-to-image fashion search engine\n",
    "- [DALL-E Flow](https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb#scrollTo=NeWDy9viOCAP)/[Disco Art](https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb#scrollTo=47428f37) - create AI-generated art in your browser"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "stack_overflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2062610f348d40dbbdf140a27509b4a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b35499811e94bee8c2692361f4d9915": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_c66e25413bb24fce843e44795e4cc25c",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠸</span> Waiting <span style=\"font-weight: bold\">summary</span>... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">2/3</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n",
         "text/plain": "\u001b[32m⠸\u001b[0m Waiting \u001b[1msummary\u001b[0m... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m \u001b[33m0:00:01\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "c66e25413bb24fce843e44795e4cc25c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fea82d7b793a4beb987f541c61b39251": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_2062610f348d40dbbdf140a27509b4a8",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠸</span> Waiting <span style=\"font-weight: bold\">summary</span>... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">2/3</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n",
         "text/plain": "\u001b[32m⠸\u001b[0m Waiting \u001b[1msummary\u001b[0m... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m \u001b[33m0:00:01\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
