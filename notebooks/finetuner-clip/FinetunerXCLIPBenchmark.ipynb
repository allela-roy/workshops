{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da795a9e9c8c47c489b520483c95713a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f33b28cfdc71491baa49f843b1f51389",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠋\u001b[0m \u001b[1mDownloading\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m950009856/953017547\u001b[0m • \u001b[31m15933119 QPS\u001b[0m • \u001b[36m0:00:01\u001b[0m • \u001b[1;34m953.0 MB\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> <span style=\"font-weight: bold\">Downloading</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">950009856/953017547</span> • <span style=\"color: #800000; text-decoration-color: #800000\">15933119 QPS</span> • <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> • <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">953.0 MB</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f33b28cfdc71491baa49f843b1f51389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuner X CLIP Benchmark\n",
        "\n",
        "@bo_wangbo\n",
        "@fissoreg\n",
        "\n",
        "Traditionally, searching images from text (text-to-image retrieval) relies on human annotations. The [OpenAI CLIP](https://github.com/openai/CLIP) model instead maps texts and images into dense vectors in the same semantic space, making it possible to directly measure the similarity of texts to images.\n",
        "\n",
        "Following this, LAION AI has produced the [CLIP benchmark](https://github.com/LAION-AI/CLIP_benchmark) in order to evaluate the performance of CLIP models on various standard datasets, such as `Flickr8k`, `Flickr30k` and `MS-Coco Captions`. These datasets consist of a set of images and each image is linked to five text descriptions.\n",
        "\n",
        "In this Colab notebook, we'll try to use [Finetuner](https://github.com/jina-ai/finetuner) to fine-tune the CLIP model on `Flickr8k`, and compare the retrieval metrics produced by the fine-tuned model against pre-trained zero-shot results produced from CLIP Benchmark.\n",
        "\n",
        "*NOTE: Finetuner is a cloud-based training platform, which requires you to login and Finetuner will allocate computational resources automatically for free.*\n",
        "\n",
        "**Please Consider [Switch to GPU Runtime](https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403) for faster evaluation!**\n",
        "\n",
        "![flickr](https://miro.medium.com/max/1400/0*5WGNo6Ty72mdQhcK)"
      ],
      "metadata": {
        "id": "08NvzS3_nCXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"finetuner[full]==0.6.3\"\n",
        "!pip install nest_asyncio\n",
        "# our fork of CLIP benchmark, resolved some minor issues in data builder and adjust the evaluator code to allow evaluator receive 2 models\n",
        "# when fine-tuning CLIP, Finetuner will un-wrap the CLIP model into 2 models and save them individually\n",
        "!pip install kaggle\n",
        "!pip install git+https://github.com/bwanglzu/CLIP_benchmark.git"
      ],
      "metadata": {
        "id": "DFB3KT4a59mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training data\n",
        "\n",
        "CLIP Benchmark has done a lot job for us with a dataset `builder`. However, to feed the training data into Finetuner, we need to convert it into Jina DocArray format. To be more specific:\n",
        "\n",
        "1. CLIP Benchmark contains a file named `captions.txt` which includes all Flickr8k image urls with captions.\n",
        "2. CLIP Benchmark reused the Karpathy split which split the `Flickr8k` into test sets and training sets. The test set includs 5000 images with annotations.\n",
        "\n",
        "We will build our training set by loading all images then exclude all the test set images from the Karpathy split.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ol2JJrSJsD-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from clip_benchmark.datasets.builder import build_dataset\n",
        "\n",
        "# please fill in your kaggle token here, you should be able to get your kaggle \n",
        "# user name and key in kaggle personal settings.\n",
        "# CLIP Benchmark uses kaggle to download flickr8k dataset\n",
        "os.environ['KAGGLE_USERNAME'] = ''\n",
        "os.environ['KAGGLE_KEY'] = ''\n",
        "\n",
        "build_dataset(dataset_name='flickr8k', annotation_file=None, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUbvP-jGD74b",
        "outputId": "2db3f3b5-fe80-4384-b694-5f96bc3d08c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset Flickr\n",
              "    Number of datapoints: 1000\n",
              "    Root location: root"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/root/'\n",
        "full_annotation = root_dir + 'captions.txt'\n",
        "test_annotation = root_dir + 'flickr8k_test_karpathy.txt'\n",
        "\n",
        "all_imgs = []\n",
        "test_imgs = []\n",
        "with open(full_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, item in enumerate(f.readlines()):\n",
        "        all_imgs.append(item.split(',', 1)[0])\n",
        "\n",
        "with open(test_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, item in enumerate(f.readlines()):\n",
        "        test_imgs.append(item.split(',', 1)[0])\n",
        "\n",
        "print(f'Size of the full image set is {len(all_imgs)}')\n",
        "print(f'Size of the test image set is {len(test_imgs)}')"
      ],
      "metadata": {
        "id": "a8RAM-4UD9aF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15558fce-9166-4827-9985-1134f617954c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the full image set is 40455\n",
            "Size of the test image set is 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will convert the downloaded images into `DocumentArray` format. The essentials of `DocumentArray` are as follows:\n",
        "\n",
        "+ `docarray` is a dependency of Finetuner, so no need to install it separately.\n",
        "+ A `Document` wraps a piece of data of any type -- be image, text or anything.\n",
        "+ A `DocumentArray` wraps a list of `Document`s.\n",
        "+ When fine-tuning CLIP, our training data consist pairs of one image and one text.\n",
        "\n",
        "We organize a training set like this:\n",
        "\n",
        "```python\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "pairs = DocumentArray()\n",
        "pair_1 = Document(chunks=[\n",
        "    img_chunk = Document(uri='your-image.jpg', modality='image'),\n",
        "    txt_chunk = Document(content='the text descriptor', modality='text'),\n",
        "]}\n",
        "pair_2 = ...\n",
        "pairs.extend([pair_1, pair_2, ...])\n",
        "```"
      ],
      "metadata": {
        "id": "PG3M2Cv7wfKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "train = DocumentArray()\n",
        "with open(full_annotation, 'r') as f:\n",
        "    next(f) # exclude the header line\n",
        "    for idx, line in tqdm(enumerate(f.readlines())):\n",
        "        url, txt = line.split(',', 1)\n",
        "        if url in test_imgs:  # do not include test images into training set\n",
        "            continue\n",
        "        img_chunk = Document(uri=root_dir + url, modality='image')\n",
        "        txt_chunk = Document(content=txt, modality='text')\n",
        "        img_chunk.load_uri_to_image_tensor(224, 224)\n",
        "        img_chunk.pop('uri')\n",
        "        pair = Document(chunks=[img_chunk, txt_chunk])\n",
        "        train.append(pair)\n",
        "        if idx == 5000: # we only use a subset to train\n",
        "            break\n",
        "\n",
        "print(f'The size of the training data is {len(train)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLgNsciPvLlH",
        "outputId": "77538bb0-d0d8-4959-f723-43ace87630d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [00:30, 166.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the training data is 4376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flickr8k dataset contains 8,000 images, each with 5 descriptive texts, or 40,000 image-text pairs in total.\n",
        "\n",
        "+ The training set has ~35000 image-text pairs.\n",
        "+ The test set has ~5000 image-text pairs.\n",
        "\n",
        "## Start Fine-tuning\n",
        "\n",
        "Now we have everything ready, the next step is to start the fine-tuning job using Finetuner. What Finetuner doing is it takes pre-trained model from a 3rd party library, such as `open_clip`, then jointly optimize the `CLIPLoss`function for the image encoder and text encoder.\n",
        "\n",
        "Finetuner will also reserve a cloud GPU for you for free."
      ],
      "metadata": {
        "id": "FhXVm8tV0VEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import finetuner\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply() # to execute async function in the notebook\n",
        "finetuner.login()\n",
        "# please copy the link below into a new tab in order to login."
      ],
      "metadata": {
        "id": "hG_siRL7zvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note, we have push the training set below to the cloud, and set the dataset as public, so you don't have to push again.\n",
        "# train.push('finetuner-flickr8k-demo', public=True, show_progress=True)\n",
        "# finetuner.delete_run('clip-run')"
      ],
      "metadata": {
        "id": "KrFZqjvY26eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = finetuner.fit(\n",
        "    model='ViT-B-32#openai', # we take ViT-B-32 trained from Open AI, model provided by OpenCLIP\n",
        "    train_data='finetuner-flickr8k-demo', # the dataset we prepared has been pushed to the cloud in the prev section\n",
        "    run_name='clip-run',\n",
        "    loss='CLIPLoss', # use CLIPLoss for fine-tuning CLIP model\n",
        "    epochs=5,\n",
        "    learning_rate= 1e-6,\n",
        "    cpu=False,\n",
        ")"
      ],
      "metadata": {
        "id": "O7BbFKF71c-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes around ~10 minutes to finish\n",
        "for log_entry in run.stream_logs():\n",
        "    print(log_entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kGVQAWnh25SK",
        "outputId": "9fa85d03-4240-4b07-ded4-40949e7604bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠸</span> Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m⠸\u001b[0m Preparing to run, logs will be ready to pull when `status` is `STARTED`. Current status is `CREATED`\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15:42:58] INFO     Starting finetuner run ...                                                           __main__.py:112\n",
            "           DEBUG    Found Hubble authentication token                                                    __main__.py:124\n",
            "           DEBUG    Running in online mode                                                               __main__.py:125\n",
            "           INFO     Reading config ...                                                                   __main__.py:132\n",
            "           DEBUG    Reading config from stream                                                           __main__.py:144\n",
            "           INFO     Parsing config ...                                                                   __main__.py:147\n",
            "           INFO     Config loaded 📜                                                                     __main__.py:149\n",
            "           INFO     Run name: clip-run                                                                   __main__.py:151\n",
            "           INFO     Experiment name: content                                                             __main__.py:152\n",
            "           DEBUG    Device set to [cuda]                                                                 __main__.py:155\n",
            "           DEBUG    Artifact ID set to 6340481f6b5bbe770d7383ca                                          __main__.py:163\n",
            "           INFO     Bulding the tuner components ...                                                     __main__.py:165\n",
            "           INFO     Building models ...                                                                    parser.py:194\n",
            "           DEBUG    Model name: ViT-B-32#openai                                                            parser.py:196\n",
            "           DEBUG    Model options: {}                                                                      parser.py:197\n",
            "[15:42:59] INFO     Created a temporary directory at /tmp/tmpeq4h39os                                 instantiator.py:21\n",
            "           INFO     Writing /tmp/tmpeq4h39os/_remote_module_non_scriptable.py                         instantiator.py:76\n",
            "           INFO     Loading pretrained ViT-B-32 from OpenAI.                                               factory.py:72\n",
            "\n",
            "  0%|                                               | 0.00/354M [00:00<?, ?iB/s]\n",
            "  4%|█▍                                     | 13.0M/354M [00:00<00:02, 130MiB/s]\n",
            "  8%|██▉                                    | 26.7M/354M [00:00<00:02, 134MiB/s]\n",
            " 12%|████▌                                  | 41.4M/354M [00:00<00:02, 140MiB/s]\n",
            " 16%|██████▏                                | 56.2M/354M [00:00<00:02, 143MiB/s]\n",
            " 20%|███████▊                               | 70.7M/354M [00:00<00:01, 143MiB/s]\n",
            " 24%|█████████▎                             | 85.0M/354M [00:00<00:02, 133MiB/s]\n",
            " 28%|██████████▉                            | 99.4M/354M [00:00<00:01, 136MiB/s]\n",
            " 32%|████████████▊                           | 114M/354M [00:00<00:01, 138MiB/s]\n",
            " 36%|██████████████▍                         | 128M/354M [00:00<00:01, 140MiB/s]\n",
            " 40%|████████████████▏                       | 143M/354M [00:01<00:01, 143MiB/s]\n",
            " 44%|█████████████████▊                      | 157M/354M [00:01<00:01, 142MiB/s]\n",
            " 49%|███████████████████▍                    | 172M/354M [00:01<00:01, 145MiB/s]\n",
            " 53%|█████████████████████▏                  | 187M/354M [00:01<00:01, 144MiB/s]\n",
            " 57%|██████████████████████▊                 | 201M/354M [00:01<00:01, 102MiB/s]\n",
            " 60%|███████████████████████▌               | 213M/354M [00:01<00:02, 65.6MiB/s]\n",
            " 64%|█████████████████████████▏             | 228M/354M [00:02<00:01, 79.6MiB/s]\n",
            " 68%|██████████████████████████▎            | 239M/354M [00:04<00:06, 18.3MiB/s]\n",
            " 70%|███████████████████████████▏           | 247M/354M [00:04<00:05, 19.0MiB/s]\n",
            " 74%|████████████████████████████▊          | 261M/354M [00:04<00:03, 27.1MiB/s]\n",
            " 78%|██████████████████████████████▎        | 276M/354M [00:04<00:02, 37.0MiB/s]\n",
            " 82%|███████████████████████████████▉       | 290M/354M [00:04<00:01, 48.6MiB/s]\n",
            " 85%|█████████████████████████████████▎     | 302M/354M [00:04<00:01, 50.3MiB/s]\n",
            " 88%|██████████████████████████████████▎    | 312M/354M [00:06<00:02, 20.6MiB/s]\n",
            " 90%|███████████████████████████████████    | 319M/354M [00:06<00:02, 16.4MiB/s]\n",
            " 94%|████████████████████████████████████▌  | 332M/354M [00:07<00:00, 23.7MiB/s]\n",
            " 98%|██████████████████████████████████████▏| 346M/354M [00:07<00:00, 33.3MiB/s]\n",
            "100%|███████████████████████████████████████| 354M/354M [00:07<00:00, 48.0MiB/s]\n",
            "[15:43:10] INFO     Loading pretrained ViT-B-32 from OpenAI.                                               factory.py:72\n",
            "[15:43:13] INFO     Building loss ...                                                                      parser.py:257\n",
            "           DEBUG    Loss name: CLIPLoss                                                                    parser.py:258\n",
            "           INFO     Building optimizer ...                                                                 parser.py:303\n",
            "           DEBUG    Optimizer name: Adam                                                                   parser.py:304\n",
            "           DEBUG    Optimizer options: {'lr': 1e-06}                                                       parser.py:305\n",
            "           INFO     Loading the training data ...                                                          parser.py:113\n",
            "           INFO     Pulling data from cloud storage ...                                                     parser.py:91\n",
            "🔐 You are logged in to Jina AI as bo.wang. To log out, use jina auth logout.\n",
            "ping\n",
            "2022-10-07 15:43:31.215019\n",
            "ping\n",
            "2022-10-07 15:44:01.218151\n",
            "ping\n",
            "2022-10-07 15:44:31.221405\n",
            "ping\n",
            "2022-10-07 15:45:01.223902\n",
            "  Deserializing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4376/4376 • 28 QPS • 0:00:00 • 545.2 MB\n",
            "[15:44:51] DEBUG    # training samples: 4376                                                               parser.py:120\n",
            "           INFO     Model, data, callbacks, loss, miner and optimizer built successfully 🔥              __main__.py:168\n",
            "           INFO     Finetuning ...                                                                       __main__.py:173\n",
            "ping\n",
            "2022-10-07 15:45:31.225925\n",
            "ping\n",
            "2022-10-07 15:46:01.229435\n",
            "ping\n",
            "2022-10-07 15:46:31.231385\n",
            "ping\n",
            "2022-10-07 15:47:01.235305\n",
            "ping\n",
            "2022-10-07 15:47:31.237492\n",
            "ping\n",
            "2022-10-07 15:48:01.241156\n",
            "ping\n",
            "2022-10-07 15:48:31.244017\n",
            "ping\n",
            "2022-10-07 15:49:01.245765\n",
            "  Training [5/5] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 0:00:00 0:00:50 • loss: 0.200\n",
            "[15:49:04] INFO     Done ✨                                                                              __main__.py:194\n",
            "           DEBUG    Finetuning took 0 days, 0 hours 4 minutes and 13 seconds                             __main__.py:196\n",
            "           INFO     Building the artifact ...                                                            __main__.py:207\n",
            "           INFO     Pushing artifact to Hubble ...                                                       __main__.py:231\n",
            "ping\n",
            "2022-10-07 15:49:31.248001\n",
            "ping\n",
            "2022-10-07 15:50:01.250263\n",
            "ping\n",
            "2022-10-07 15:50:31.253734\n",
            "ping\n",
            "2022-10-07 15:51:01.255555\n",
            "ping\n",
            "2022-10-07 15:51:31.260043\n",
            "ping\n",
            "2022-10-07 15:52:01.262100\n",
            "ping\n",
            "2022-10-07 15:52:31.264475\n",
            "ping\n",
            "2022-10-07 15:53:01.266802\n",
            "ping\n",
            "2022-10-07 15:53:31.269136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "After fine-tuning is finished, your fine-tuned model is saved in the cloud as an `artifact`. An `artifact` contains the model weights, and some metadata such as evaluation metrics and hyper-parameters.\n",
        "\n",
        "In order to download your artifact, call the method `run.save_artifact()`.\n",
        "\n",
        "Since CLIP is actually two models and we are fine-tuning them in parallel, there will be two models downloaded as one artifact: a text encoder and an image encoder. To use these models to do encodings, you will need the `finetuner.get_model()` with a `select_model` -- either `clip-text` or `clip-vision` -- get access to CLIPs constituent models individually."
      ],
      "metadata": {
        "id": "gV5tKv0sUoBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artifact = run.save_artifact('clip-model')\n",
        "\n",
        "clip_txt_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
        "clip_img_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "da795a9e9c8c47c489b520483c95713a",
            "f33b28cfdc71491baa49f843b1f51389"
          ]
        },
        "id": "Do1nK3sY2OS7",
        "outputId": "50a6b94b-335b-4427-dcfd-9a74e8787b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da795a9e9c8c47c489b520483c95713a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 354M/354M [00:01<00:00, 267MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these two models and Finetuner, you can encode your image and text data with:\n",
        "\n",
        "```python\n",
        "data = DocumentArray([Document(content='some text to encode')])\n",
        "finetuner.encode(model=clip_txt_encoder, data=data)\n",
        "```"
      ],
      "metadata": {
        "id": "qnlCnAiPZLjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS WILL BE REPLACED\n",
        "\n",
        "In order to re-used the code from CLIP Benchmark, we need to adjust a bit to get the re-build model ourselves and load the weights."
      ],
      "metadata": {
        "id": "gRDeIQRe2aiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "note these are installed along with the finetuner package\n",
        "while not intented to become public methods.\n",
        "You can use finetuner.encode(model, data) to encode your DocumentArray\n",
        "In order to re-use CLIP Benchmark, we used a \"hacky\" way to build models as below\n",
        "'''\n",
        "!unzip clip-model/clip-run.zip # as said, artifact are saved as zip together with weights and some metadata.\n",
        "import torch\n",
        "from commons.models.builders import OpenCLIPVisionBuilder, OpenCLIPTextBuilder\n",
        "\n",
        "clip_vision = OpenCLIPVisionBuilder(descriptor='ViT-B-32#openai').build()\n",
        "clip_vision.load_state_dict(torch.load(f'/content/{run.name}/models/clip-vision/model.pt'))\n",
        "\n",
        "clip_text = OpenCLIPTextBuilder(descriptor='ViT-B-32#openai').build()\n",
        "clip_text.load_state_dict(torch.load(f'/content/{run.name}/models/clip-text/model.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q503pCw4bS4E",
        "outputId": "a2c85428-bfe5-4c87-8e2a-46918a509f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Console script for clip_benchmark. \n",
        "Code copied from CLIP Benchmark with minor adjusts to run in colab.\n",
        "\"\"\"\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import open_clip\n",
        "from pprint import pprint\n",
        "\n",
        "from clip_benchmark.datasets.builder import build_dataset, get_dataset_collate_fn\n",
        "from clip_benchmark.metrics import  zeroshot_retrieval\n",
        "\n",
        "from torch.utils.data import default_collate\n",
        "\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "image_encoder = clip_vision.to(device)\n",
        "text_encoder = clip_text.to(device)\n",
        "_, _, transform = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "dataset = build_dataset(\n",
        "    dataset_name='flickr8k',\n",
        "    root='root',\n",
        "    transform=transform,\n",
        "    split='test',\n",
        "    annotation_file=None,\n",
        "    download=True,\n",
        ")\n",
        "collate_fn = get_dataset_collate_fn('flickr8k')\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=64,\n",
        "    shuffle=False, num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "metrics = zeroshot_retrieval.evaluate(\n",
        "    image_encoder,\n",
        "    text_encoder,\n",
        "    dataloader,\n",
        "    open_clip.tokenizer.tokenize,\n",
        "    recall_k_list=[5],\n",
        "    device=device,\n",
        "    amp=True\n",
        ")\n",
        "dump = {\n",
        "    \"dataset\": 'flickr8k',\n",
        "    \"model\": 'ViT-B-32',\n",
        "    \"pretrained\": 'openai',\n",
        "    \"task\": 'finetuned',\n",
        "    \"metrics\": metrics\n",
        "}\n",
        "pprint(dump)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnbGx73pbxFU",
        "outputId": "d1f01226-28af-45e5-c7a3-aeeeecc03460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "16it [00:16,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dataset': 'flickr8k',\n",
            " 'metrics': {'image_retrieval_recall@5': 0.8537999987602234,\n",
            "             'text_retrieval_recall@5': 0.9100000262260437},\n",
            " 'model': 'ViT-B-32',\n",
            " 'pretrained': 'openai',\n",
            " 'task': 'finetuned'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot VS Fine-Tuned\n",
        "\n",
        "TO REVISE WITH BO AFTER RELEASE\n",
        "\n",
        "CLIP Benchmark has done a lot of experiments and publihsed their results in [this csv](https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/benchmark.csv).\n",
        "\n",
        "For simplicity, we show the comparsion below:\n",
        "\n",
        "+ `image_retrieval_recall@5`: use text queries to find top 5 similar images.\n",
        "+ `text_retrieval_recall@5`: use image to find top 5 similar text.\n",
        "\n",
        "\n",
        "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
        "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
        "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8537999987602234| 0.9100000262260437 |\n",
        "\n",
        "Apart from that, we have done some extensive experiments on three datasets, these are the results we get:\n",
        "\n",
        "\n",
        "| model                            | dataset       | imageRecall@5(zero-shot) | textRecall@5(zero-shot) | imageRecall@5(fine-tuned) | textRecall@5(fine-tuned) |\n",
        "|----------------------------------|---------------|-------------------|----------------------|---------|-------------|\n",
        "| ViT-B-32#openai                  | flickr8k      |0.5319737792015076 | 0.6991719007492065   |0.8651999831199646| 0.9079999923706055 |\n",
        "| ViT-B-16-plus-240                | flickr8k      |0.6441478133201599 | 0.7916203141212463   |0.8784000277519226| 0.9200000166893005 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | flickr8k      |0.5787171125411987 | 0.7392163872718811   |0.849399983882904 | 0.9020000100135803 |\n",
        "| ViT-B-32#openai                  | flickr30k     |0.8338000178337097 | 0.9490000009536743   |0.9016000032424927| 0.9480000138282776 |\n",
        "| ViT-B-16-plus-240                | flickr30k     |0.8894000053405762 | 0.9710000157356262   |0.9169999957084656| 0.9710000157356262 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | flickr30k     |0.8546000123023987 | 0.9409999847412109   |0.8715999722480774| 0.9290000200271606 |\n",
        "| ViT-B-32#openai                  | coco captions |0.5584565997123718 | 0.748199999332428    |0.6546581387519836| 0.7454000115394592 |\n",
        "| ViT-B-16-plus-240                | coco captions |0.6620951890945435 | 0.8101999759674072   |0.7120751738548279| 0.8136000037193298 |\n",
        "| ViT-B-32-quickgelu#laion400m_e32 | coco captions |0.6084766387939453 | 0.7675999999046326   |0.6713714599609375| 0.7635999917984009 |\n",
        "\n",
        "Default hyper-parameters are: `learning_rate: 1e-6`, `epochs: 5`, `optimizer: Adam`.\n",
        "Flickr models are fine-tuned on 8k/30k images while evaluated on the karpathy test set.\n",
        "``mscoco_captions`` models are fine-tuned on a random subset (100k pairs) extracted from `2014 train images`\n",
        "and evaluated on `2014 val images`.\n",
        "\n",
        "## General Insights when fine-tuning CLIP\n",
        "\n",
        "+ Use a small learning rate, such as 1e-5, 1e-6, 1e-7...\n",
        "+ Not necessarily need huge and complex models, based ViT-B-32 is good enough with fine-tuning.\n",
        "+ If your search case is close domain/different domain, fine-tuning might be a good idea, otherwise not.\n",
        "+ If you don't have text descriptors, only keyword, use a prompt to turn your keyword into a sentence. For instance a cat image with `cat` as label, please turn `cat` into `this is a photo of cat`. \n",
        "+ Consider [wise-ft](https://github.com/mlfoundations/wise-ft) if you do not want to loss too much zero-shot capbility.\n",
        "\n",
        "Again, if you are interested to see how Finetuner could improve your representation and apply for search/recommendation/deduplication tasks, please find us at:\n",
        "\n",
        "+ Github: https://github.com/jina-ai/finetuner\n",
        "+ Documentation: https://finetuner.jina.ai/\n",
        "\n",
        "Last but not least, huge thanks to all open source contributors:\n",
        "\n",
        "+ CLIP Benchmark: https://github.com/LAION-AI/CLIP_benchmark\n",
        "+ Open CLIP: https://github.com/mlfoundations/open_clip"
      ],
      "metadata": {
        "id": "ODbzFgVGlBd_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIPy1Bh5lxGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}