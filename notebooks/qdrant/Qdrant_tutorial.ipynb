{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5e6fed-9402-4f6d-b1e3-60fb55b0ab13",
   "metadata": {},
   "source": [
    "# Smart RAG with Jina and Qdrant\n",
    "\n",
    "This notebook will show you how to make a basic RAG engine using the [LlamaIndex framework](https://www.llamaindex.ai/), the open-source [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) LLM, [Jina Embeddings v2](https://jina.ai/embeddings), and [Qdrant’s AI-ready vector database](https://qdrant.tech/).\n",
    "\n",
    "The first step in building this RAG system is to install the prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2314c-b228-460b-886f-944b985c13a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index qdrant-client pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600c6b6-de7b-46dd-8624-2d6ac6b93f4b",
   "metadata": {},
   "source": [
    "You will need access to the Hugging Face Inference API, including an access token. If you have a Hugging Face account, you can get one from [your account settings page](https://huggingface.co/settings/tokens).\n",
    "\n",
    "If you do not have an account, first set one up [on the Hugging Face website](https://huggingface.co/join), then go to [your account settings](https://huggingface.co/settings/tokens) page to create an access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b9e7-de71-487a-9aab-2ee852f4a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_inference_api_key: str = \"<your HuggingFace Inference API token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12672597-d36b-48e3-836c-21cb4d849d5c",
   "metadata": {},
   "source": [
    "Next, we construct a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aaa25-82ec-477e-afda-c2032faf7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query. Please be brief, concise, and complete.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"No information\\\".\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ba575-ed57-49a3-962f-a137801f2d7f",
   "metadata": {},
   "source": [
    "Finally, we create and initialize an object for the LlamaIndex framework that holds the connection to Mistral-7B-Instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1cb529-9f18-4f6e-a6c9-83631d37aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from llama_index.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.llms.base import llm_completion_callback\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class MixtralLLM(CustomLLM):\n",
    "    context_window: int = 4096\n",
    "    num_output: int = 1024\n",
    "    model_name: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    api_key: str = hf_inference_api_key\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    def do_hf_call(self, prompt: str) -> str:\n",
    "        data = {\n",
    "            \"inputs\": prompt\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            'https://api-inference.huggingface.co/models/' + self.model_name,\n",
    "            headers={\n",
    "                'authorization': f'Bearer {self.api_key}',\n",
    "                'content-type': 'application/json',\n",
    "            },\n",
    "            json=data,\n",
    "            stream=True\n",
    "        )\n",
    "        if response.status_code != 200 or not response.json() or 'error' in response.json():\n",
    "            print(f\"Error: {response}\")\n",
    "            return \"Unable to answer for technical reasons.\"\n",
    "        full_txt = response.json()[0]['generated_text']\n",
    "        offset = full_txt.find(\"---------------------\")\n",
    "        ss = full_txt[offset:]\n",
    "        offset = ss.find(\"Answer:\")\n",
    "        return ss[offset+7:].strip()\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = self.do_hf_call(prompt)\n",
    "        return CompletionResponse(text=response)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "            self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        response = \"\"\n",
    "        for token in self.do_hf_call(prompt):\n",
    "            response += token\n",
    "            yield CompletionResponse(text=response, delta=token)\n",
    "\n",
    "\n",
    "mixtral_llm = MixtralLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46e198-8520-47bf-9d40-ede73ff7c095",
   "metadata": {},
   "source": [
    "Finally, we create and initialize an object for the LlamaIndex framework that holds the connection to Mistral-7B-Instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ce895-38f9-4b03-8c8c-891ee0c79888",
   "metadata": {},
   "source": [
    "# Information Retrieval with Qdrant and Jina Embeddings\n",
    "\n",
    "To set up the retrieval system, you will need a Jina Embeddings API key. You can get one with one million free tokens at the [Jina Embeddings website](https://jina.ai/embeddings/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6703d10-25d3-4aa5-9182-345abb5f0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_emb_api_key = \"<your Jina Embeddings API key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee782c-44da-4fb9-9425-d8dc6a5d27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    api_key=jina_emb_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf93d0-9603-4be3-9485-bcbac25a9b53",
   "metadata": {},
   "source": [
    "Then, create a connector object using LlamaIndex for the Jine Embeddings server, selecting specifically the English monolingual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001f42c-2435-4a1d-950c-2e85cdbf022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "uri = \"https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf\"\n",
    "pdf_data = urllib.request.urlopen(uri).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45415a-a282-42eb-8604-69d32a0f53c9",
   "metadata": {},
   "source": [
    "## Load text data\n",
    "\n",
    "Next, we will load the document and split it up into paragraphs. First, download the PDF from the White House website into the variable pdf_data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8cb4f-9359-4acf-8bdd-c886ed6a45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from io import BytesIO, StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "text_paras = []\n",
    "parser = PDFParser(BytesIO(pdf_data))\n",
    "doc = PDFDocument(parser)\n",
    "rsrcmgr = PDFResourceManager()\n",
    "for page in PDFPage.create_pages(doc):\n",
    "    output_string = StringIO()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    interpreter.process_page(page)\n",
    "    page_text = output_string.getvalue()\n",
    "    text_paras.extend(re.split(r'\\n\\s*\\n', page_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c8f4b-5310-4b75-b0b8-ab1cc9231aa5",
   "metadata": {},
   "source": [
    "Check that everything is loaded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058772fe-77e2-4ec8-a89e-c7321a7ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(text_paras) == 615"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a9040-1cb3-4eb3-a302-deed4d880d65",
   "metadata": {},
   "source": [
    "Next, we will convert this list of short texts into LlamaIndex Document objects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed8ef3-3fa2-4933-8dbf-57f059df723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import StringIterableReader\n",
    "\n",
    "rag_docs = StringIterableReader().load_data(text_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460bfb8-7983-4302-a6c2-bbd0b71dc651",
   "metadata": {},
   "source": [
    "And you can inspect the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a4f67-45bf-4105-ab7d-7c8f05cf7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NATIONAL ARTIFICIAL INTELLIGENCE \n",
      "RESEARCH AND DEVELOPMENT \n",
      "STRATEGIC PLAN \n",
      "2023 UPDATE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag_docs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a0173-611a-46ca-b1f6-15582a4d1d2d",
   "metadata": {},
   "source": [
    "## Set up a Qdrant Vector Database\n",
    "\n",
    "You will need to create an account on the [Qdrant Cloud website](https://cloud.qdrant.io/login) before continuing.\n",
    "\n",
    "Once you have an account and are logged in, you will need to create a cluster.\n",
    "\n",
    "Follow the [“quick start” instructions on the Qdrant Website](https://qdrant.tech/documentation/cloud/quickstart-cloud/) to set up a free cluster and get an API and the name of the Qdrant host server name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8bb7d-b0f4-4765-8a0b-3069ceed0aa3",
   "metadata": {},
   "source": [
    "Store the key and hostname in variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cc13d-977a-4568-915e-e2c1f03710b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_api_key = \"<your API key>\"\n",
    "qdrant_server = \"https://<your server>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92f361-8ec1-40ec-8a60-3a593d373ec4",
   "metadata": {},
   "source": [
    "Next, we will need to import the relevant components from the `qdrant_client` and `llama_index` packages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871450a-e18b-4ee2-982f-cb9b56c9a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"NTSC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f43e20-f0b8-46f7-9d2f-fcba6fd155aa",
   "metadata": {},
   "source": [
    "This creates a collection named `NTSC` in your free cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd9ebc-7b16-49f0-b858-92d5f242173a",
   "metadata": {},
   "source": [
    "## Complete the full RAG system\n",
    "\n",
    "Now we will assemble these components into a complete RAG system using boilerplate code for LlamaIndex. This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf3254-6935-4ef8-9775-8acd3f931f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import (\n",
    "\t\tVectorStoreIndex,\n",
    "\t\tServiceContext,\n",
    "\t\tget_response_synthesizer,\n",
    ")\n",
    "\n",
    "# set up the service and storage contexts\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create an index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    rag_docs, storage_context=storage_context, service_context=service_context\n",
    ")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228bb09-20e2-42f6-847f-06919118e0cc",
   "metadata": {},
   "source": [
    "Now the system is ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07c103-d243-4bb2-b261-85989afd80d8",
   "metadata": {},
   "source": [
    "# Querying a Document\n",
    "\n",
    "Let’s try a straightforward query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abc6b5-9930-40a9-bafe-7aa9fecfe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "What is the Biden Administration's policy with regard to AI? \n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee3835-ba2d-461d-a8eb-35d4f48df855",
   "metadata": {},
   "source": [
    "Or something more specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9fa3b-a405-4bb7-a94b-4ec723393f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "What protections does the AI Bill of Rights propose to offer?\n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef391a7-e768-4ad8-880c-11489a773872",
   "metadata": {},
   "source": [
    "Or even very specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c39fb3-10c2-413b-9375-2a27ecfc2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is Kei Koizumi?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114ba00-4641-4f1a-b631-701f9e62555b",
   "metadata": {},
   "source": [
    "You can also ask more fanciful questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c8fad-15b2-421b-8ec4-6c4451e728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "What rights will AIs receive under President Biden's proposed \n",
    "AI Bill of Rights?\n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1bf4d-2a0e-49a8-ba19-3fb3d853cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "Why is President Biden's proposing an AI Bill of Rights?\n",
    "Does AI really need rights?\n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f5fd4-02d9-49e8-bdb0-e6724d878d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "Has Donald Trump weighed in on AI?\n",
    "Will he Make Humans Great Again?\n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdrant_venv",
   "language": "python",
   "name": "qdrant_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
