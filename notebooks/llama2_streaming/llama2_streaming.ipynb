{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Streaming API for LlaMa 2: Real-time AI with Jina and DocArray\n",
        "\n",
        "[Read the accompanying article on the Jina AI website.](https://jina.ai/news/building-a-streaming-api-for-llama-2-real-time-ai-with-jina-and-docarray)\n",
        "\n",
        "⚠ Activate GPU in order to run the notebook. (Click on **Runtime** > **Change runtime type**, and in the popup, select **GPU** under **Hardware accelerator**.)\n",
        "\n",
        "First, we install the libraries we need."
      ],
      "metadata": {
        "id": "e6Sv3Y0bC8ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docarray jina bitsandbytes accelerate transformers"
      ],
      "metadata": {
        "id": "8qmJvHc1SStS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠ Before starting, you will need permission from Meta to download Llama 2 models from Hugging Face.\n",
        "\n",
        "- First, you will need a Hugging Face account. You can sign up for one for free at https://huggingface.co/join.\n",
        "- Next, you will need to request permission from Meta. You can make the request at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf when you are logged in with your Hugging Face credentials. This request may take **one to two days to process**.\n",
        "- Finally, request a token from Hugging Face from your settings page at https://huggingface.co/settings/tokens. You will need this token to download the model.- Finally, request a token from Hugging Face from your settings page at https://huggingface.co/settings/tokens. You will need this token to download the model.\n",
        "\n",
        "**Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.**\n",
        "\n",
        "When prompted by `huggingface-cli` below, paste your token into the `Token:` field and press enter."
      ],
      "metadata": {
        "id": "auuo5dBcDZrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "5syCHjzKFwsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the schema classes\n",
        "\n",
        "First, we write the message schemas using DocArray."
      ],
      "metadata": {
        "id": "ywwBjjUEhHdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docarray import BaseDoc\n",
        "\n",
        "class PromptDocument(BaseDoc):\n",
        "    prompt: str\n",
        "    max_tokens: int\n",
        "\n",
        "class ModelOutputDocument(BaseDoc):\n",
        "    token_id: int\n",
        "    generated_text: str"
      ],
      "metadata": {
        "id": "MRosCZw1EmLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct an Executor\n",
        "\n",
        "Now, construct the Executor, including the code to download and open the Llama-2-7b-chat-hf model and tokenizer."
      ],
      "metadata": {
        "id": "6wbe92CHjx40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jina import Executor, requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "class TokenStreamingExecutor(Executor):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map='auto',\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "    def starts_with_space(self, token_id):\n",
        "        token = self.tokenizer.convert_ids_to_tokens(token_id)\n",
        "        return token.startswith('▁')\n",
        "\n",
        "    @requests(on='/stream')\n",
        "    async def task(self, doc: PromptDocument, **kwargs) -> ModelOutputDocument:\n",
        "        input = self.tokenizer(doc.prompt, return_tensors='pt')\n",
        "        input_len = input['input_ids'].shape[1]\n",
        "\n",
        "        for output_length in range(doc.max_tokens):\n",
        "            output = self.model.generate(**input, max_new_tokens=1)\n",
        "            current_token_id = output[0][-1]\n",
        "            if current_token_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            current_token = self.tokenizer.decode(current_token_id, skip_special_tokens=True)\n",
        "            if self.starts_with_space(current_token_id.item()) and output_length > 1:\n",
        "                current_token = ' ' + current_token\n",
        "            yield ModelOutputDocument(\n",
        "                token_id=current_token_id,\n",
        "                generated_text=current_token,\n",
        "            )\n",
        "\n",
        "            input = {\n",
        "                'input_ids': output,\n",
        "                'attention_mask': torch.ones(1, len(output[0])),\n",
        "            }\n"
      ],
      "metadata": {
        "id": "t-E12PNtoBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a prompt for Llama 2.\n",
        "\n",
        "Next, we create the prompt, including system instruction."
      ],
      "metadata": {
        "id": "0Nqneio4j3PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_prompt = PromptDocument(\n",
        "    max_token=200,\n",
        "    prompt=\"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful, and honest assistant. Always answer as helpfully\n",
        "and thoroughly as possible, while being safe. Sometimes people might say things\n",
        "jokingly so don't take everything too seriously.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why\n",
        "instead of answering something not correct. If you don't know the answer to a\n",
        "question, please don't share false information.\n",
        "<</SYS>>\n",
        "\n",
        "If I punch myself in the face and it hurts, am I weak or strong?! [/INST]\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "1OiZPVYMg13B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model and executing requests to it\n",
        "\n",
        "This `main()` method will create a client to talk the to deployed Executor and query it with the prompt, printing out its answer as it streams in."
      ],
      "metadata": {
        "id": "3zKNye6akL1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jina import Deployment, Client\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "  client = Client(port=12345, protocol='grpc', asyncio=True)\n",
        "  async for doc in client.stream_doc(\n",
        "      on='/stream',\n",
        "      inputs=PromptDocument(prompt=llama_prompt, max_tokens=200),\n",
        "      return_type=ModelOutputDocument,\n",
        "  ):\n",
        "      print(doc.generated_text, end='')\n"
      ],
      "metadata": {
        "id": "dyt2bqw4pKp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, we deploy the Executor and run the `main()` method from above."
      ],
      "metadata": {
        "id": "Ke8HqUO7kpUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def deploy_and_run():\n",
        "    with Deployment(uses=TokenStreamingExecutor, port=12345, protocol='grpc'):\n",
        "        await main()"
      ],
      "metadata": {
        "id": "SG4Zva1jXrnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be aware that the initial run may take 10-15 minutes due to the model download.\n",
        "\n",
        "Ideally, you should load and deploy the model once, then send requests. But for this notebook demo, we'll execute all steps together, which might slow things down."
      ],
      "metadata": {
        "id": "zvlAId4EFmOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await deploy_and_run()\n"
      ],
      "metadata": {
        "id": "Q27NKqbltIwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d05gXBq1uaCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}