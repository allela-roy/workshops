{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crossmodal_ecommerce_search_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vpYiGEg2hwkC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Use CLIP to search product photos using text or image\n",
        "\n",
        "Have you ever had a wonderful pair of shoes that were just getting worn out, and you wanted to find another similar pair? Sometimes it's tough to know exactly what words to use.\n",
        "\n",
        "[Amazon has a feature that lets you snap a picture and find similar items in their store](https://www.hunker.com/13771848/amazon-photo-search). That's great for Jeff Bezos, but not so good if you have your own ecommerce store and want that feature.\n",
        "\n",
        "So in this notebook we're going to build a simple search engine that will let you **use images (or text) to search through a catalog of fashion items**.\n",
        "\n",
        "In later notebooks we'll flesh it out with endpoints (for remote access), scaling, fine-tuning and cloud deployment. But for now let's keep it dead simple.\n",
        "\n",
        "You can find a fuller version of this example [here](https://examples.jina.ai/fashion?utm_source=notebook-ecommerce-1).\n",
        "\n",
        "## üìë Our dataset\n",
        "\n",
        "We're using a small subset of the [Kaggle Fashion Product Images (small) dataset](https://www.kaggle.com/paramaggarwal/fashion-product-images-small). The full dataset (while still small) is less practical to get into our notebook environment.\n",
        "\n",
        "It consists of \n",
        "\n",
        "- A CSV file that contains image ids and metadata\n",
        "- 770 low-res color photos of fashion items like so:\n",
        "\n",
        "|  |  | |\n",
        "| --- | --- | --- |\n",
        "| ![](https://github.com/jina-ai/example-multimodal-fashion-search/blob/main/data/subset/10001.jpg?raw=true) | ![](https://github.com/jina-ai/example-multimodal-fashion-search/blob/main/data/subset/10008.jpg?raw=true) | ![](https://github.com/jina-ai/example-multimodal-fashion-search/blob/main/data/subset/10009.jpg?raw=true) |\n",
        "\n",
        "## ü§ñ Our tech stack\n",
        "\n",
        "### DocArray\n",
        "\n",
        "We'll use [DocArray](https://docarray.jina.ai?utm_source=notebook-ecommerce-1) to convert our CSV file and images into Documents that we can then search through.\n",
        "\n",
        "DocArray is a good fit for this, since whatever kind of data we use (text or image in our case), we only need to think about one data format: A Document can contain pretty much anything. We can also use DocArray to [bulk apply]() processing on our images, which is much faster than messing around with for-loops.\n",
        "\n",
        "We can also use it to [push our dataset to the cloud](https://docarray.jina.ai/fundamentals/documentarray/serialization/#from-to-cloud?utm_source=notebook-ecommerce-1), making it easier to use it later in other notebooks.\n",
        "\n",
        "### CLIP-as-service\n",
        "\n",
        "We'll use CLIP-as-service to quickly generate [vector embeddings](https://docarray.jina.ai/fundamentals/document/embedding?utm_source=notebook-ecommerce-1) for our images, and thus be able to perform nearest-neighbor search using text or images as input.\n",
        "\n",
        "CLIP-as-service is a good match for this, since it requires minimal dependencies and has pretty low-latency. It's also free to use via Jina's cloud servers.\n",
        "\n",
        "## üí¨ Talk to us!\n",
        "\n",
        "Want to find out more about neural search and the Jina AI ecosystem? Join us on [Slack](https://slack.jina.ai?utm_source=notebook-ecommerce-1)!\n",
        "\n",
        "## üìù Notes\n",
        "\n",
        "- This is just part one in a multi-part series, where we'll use the Jina ecosystem to build an advanced text/image search engine for products."
      ],
      "metadata": {
        "id": "-p2b14uivHfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and processing our data\n",
        "\n",
        "This is just some rough and ready code to download a toy dataset and unzip it to our `data` directory:"
      ],
      "metadata": {
        "id": "vpYiGEg2hwkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/data\""
      ],
      "metadata": {
        "id": "MDFHq7kqeJfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data"
      ],
      "metadata": {
        "id": "vnwbnLhRbT7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(data_dir):\n",
        "  os.makedirs(data_dir)\n",
        "\n",
        "  os.chdir(data_dir)\n",
        "  !wget -q https://github.com/jina-ai/example-multimodal-fashion-search/raw/main/data/subset/fashion_subset.csv\n",
        "  !wget -q https://github.com/jina-ai/example-multimodal-fashion-search/raw/main/data/subset/fashion_subset.zip\n",
        "  !unzip -qq fashion_subset.zip\n",
        "  print(\"Downloaded!\")\n",
        "else:\n",
        "  print(\"Data dir already exists, skipping download!\")\n",
        "\n",
        "os.chdir(\"/content\")"
      ],
      "metadata": {
        "id": "Fvg6neBGaEGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at our CSV to see what kind of data we have:"
      ],
      "metadata": {
        "id": "wg1HJ6-He1w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 data/fashion_subset.csv"
      ],
      "metadata": {
        "id": "fPic_c3Pe5RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Documents\n",
        "\n",
        "Every piece of data we work with in the [Jina ecosystem](https://github.com/jina-ai?utm_source=notebook-ecommerce-1) has to be in the form of a [Document](https://docarray.jina.ai/fundamentals/document?utm_source=notebook-ecommerce-1) or [DocumentArray](https://docarray.jina.ai/fundamentals/documentarray?utm_source=notebook-ecommerce-1). This means that whether we're dealing with text, images, audio, or whatever, we only have one data format to keep in mind.\n",
        "\n",
        "Instead of manually creating all of our Documents we can simply load them from a CSV file with the [`from_csv()` method](https://docarray.jina.ai/datatypes/tabular?utm_source=notebook-ecommerce-1):"
      ],
      "metadata": {
        "id": "z7XIFL3csWBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "932Z095hZyBM"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq docarray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docarray import DocumentArray"
      ],
      "metadata": {
        "id": "EJzDydCjaBZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = DocumentArray.from_csv(\"./data/fashion_subset.csv\")"
      ],
      "metadata": {
        "id": "HqoLu0cEcJ5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what a DocumentArray looks like:"
      ],
      "metadata": {
        "id": "cP2DgQHTb9Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs.summary()"
      ],
      "metadata": {
        "id": "H4r0FFPxb_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, all Documents have `id` and `tags`.\n",
        "\n",
        "And now we can dig into how an individual Document looks:"
      ],
      "metadata": {
        "id": "33ISjXf_cEiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "feu3SmuzcHNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the Document's tags (metadata) were brought in directly from the CSV file.\n",
        "\n",
        "Where are the images though? Because we only took data from the CSV (which doesn't *directly* specify where the images are), we'll have to create a URI for each image under `doc.uri`.\n",
        "\n",
        "Luckily the URI is based on the `id` column of the CSV, which was mapped to `doc.id`:"
      ],
      "metadata": {
        "id": "UnMyZZ14eYjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  doc.uri = f\"{data_dir}/{doc.id}.jpg\""
      ],
      "metadata": {
        "id": "K0QVKrCyemBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're dealing with images, we'll need to:\n",
        "\n",
        "- Load the URI to an image tensor (so we have something to feed into our encoder later)\n",
        "- Resize all the images to a consistent size\n",
        "- Ensure all the tensors are in the same format\n",
        "\n",
        "We can do this in a function and then call `docs.apply(process_images)`"
      ],
      "metadata": {
        "id": "WH_y5TtoxgQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(doc):\n",
        "  return doc.load_uri_to_image_tensor().set_image_tensor_shape((80, 60))"
      ],
      "metadata": {
        "id": "QcokARKReFqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs.apply(process_images, show_progress=True)"
      ],
      "metadata": {
        "id": "zKEy3OVtfd9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now we can see each Document has an image tensor.\n",
        "\n",
        "We can see what we've got with the `plot_image_sprites()` method:"
      ],
      "metadata": {
        "id": "-d0XatTIyCqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs.plot_image_sprites()"
      ],
      "metadata": {
        "id": "Qd5phavXeYJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also normalize them for consistency (we could have done this earlier, but then the plotting doesn't look so good). There's a nice explanation of normalization [here](https://inside-machinelearning.com/en/why-and-how-to-normalize-data-object-detection-on-image-in-pytorch-part-1/#Normalizing_data)"
      ],
      "metadata": {
        "id": "a-LU1-c9yHYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  doc.set_image_tensor_normalization()"
      ],
      "metadata": {
        "id": "ZSThJ6gqfZon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating embeddings\n",
        "\n",
        "Now we've loaded our images, we can feed them into our encoder. In our cases we want to encode using the [CLIP model](https://openai.com/blog/clip?utm_source=notebook-ecommerce-1), so we can use [CLIP-as-service](https://clip-as-service.jina.ai/) since it's low-latency and simple to use.\n",
        "\n",
        "Why is CLIP a good encoder for this? CLIP encodes both text and images to a common vector space, which means we can use text to search images, images to search text, images to search images, etc.\n",
        "\n",
        "In our search engine we'll focus on:\n",
        "\n",
        "- Image-to-image\n",
        "- Text-to-image\n",
        "\n",
        "---\n",
        "\n",
        "‚ùì Want to learn more about CLIP? Check out notebook for [finetuning CLIP with anime datasets](https://colab.research.google.com/drive/189LHTpYaefMhKNIGOzTLHHavlgmoIWg9?usp=sharing)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Y0hNfpwfh3-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq clip-client"
      ],
      "metadata": {
        "id": "7S_QsBEtnbh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to set the client and encode all of our Documents. We can do this for free with Jina's CLIP-as-service server!"
      ],
      "metadata": {
        "id": "4dQvIV2bhKWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip_client import Client\n",
        "\n",
        "c = Client('grpcs://demo-cas.jina.ai:2096')\n",
        "\n",
        "docs = c.encode(docs)"
      ],
      "metadata": {
        "id": "G6t5X4BAnioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "üí° The embeddings we just generated were from a pre-trained general purpose CLIP model. That model is good for fashion products, teapots, faces, puppies, and lots of other things.\n",
        "\n",
        "For more accurate search results we'll finetune our model specifically for our fashion dataset in a future notebook. Or you can check the [finetuning CLIP for anime](https://colab.research.google.com/drive/189LHTpYaefMhKNIGOzTLHHavlgmoIWg9?usp=sharing) if you want to learn how to do that now.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9HUHkOLptAWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing our data\n",
        "\n",
        "Let's [plot out our data into a 3D graph](https://docarray.jina.ai/fundamentals/documentarray/visualization/#embedding-projector?utm_source=notebook-ecommerce-1) so we can better see how embeddings are clustered.\n",
        "\n",
        "Note: You'll need to manually interrupt the cell below when you're done by hitting the \"stop\" button. Otherwise it will block the rest of the notebook from running."
      ],
      "metadata": {
        "id": "DPeeRAh0wW78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs.plot_embeddings(image_sprites=True)"
      ],
      "metadata": {
        "id": "H-wZHJBUwjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that different types of item are in different clusters: the watches are all together, the bags are all together and so on.\n",
        "\n",
        "You may also notice that there are two groups of shirts - those with people wearing them, and those where it's just the shirt in shot. In a later notebook we'll finetune the CLIP model so all T-shirts are closer together (i.e. ignoring the humans who we don't care about in this context)"
      ],
      "metadata": {
        "id": "NI7H2cUIw3fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pushing our data to the cloud\n",
        "\n",
        "Jina allows free cloud hosting of DocumentArrays. Since we plan to use the same DocumentArray in later notebooks, let's [push it to the cloud](https://docarray.jina.ai/fundamentals/documentarray/serialization/?highlight=push%20pull#from-to-cloud?utm_source=notebook-ecommerce-1):"
      ],
      "metadata": {
        "id": "bFTgNFHvq2pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs.push(\"770-fashion-small-with-clip-embeddings\", show_progress=True)"
      ],
      "metadata": {
        "id": "JJWWj54Dq2L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search by image\n",
        "\n",
        "Let's choose one random image from our DocumentArray and use that to search for similar images in the dataset:"
      ],
      "metadata": {
        "id": "e-5iJ9HCqhHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_query = docs.sample(1)\n",
        "image_matches = docs.find(image_query)"
      ],
      "metadata": {
        "id": "x1DYovzehbsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the results. The first image will be the query image and subsequent will be matches in order of similarity.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è Because we only have a limited dataset, you may not always get something *super* similar. In the [example with the full dataset](https://examples.jina.ai/fashion?utm_source=notebook-ecommerce-1) you'll get better matches.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OsfRmTjah5eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_matches[0].plot_image_sprites()"
      ],
      "metadata": {
        "id": "vFja2ZizpM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search by text\n",
        "\n",
        "As before, we're dealing with [Documents](https://docarray.jina.ai/fundamentals/document?utm_source=notebook-ecommerce-1) and [DocumentArrays](https://docarray.jina.ai/fundamentals/documentarray?utm_source=notebook-ecommerce-1). So the text query we search with should also be wrapped in a Document."
      ],
      "metadata": {
        "id": "T--mYq9JrDFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_string = \"women's red t-shirt\"\n",
        "\n",
        "from docarray import Document\n",
        "\n",
        "text_query = DocumentArray([Document(text=query_string)])"
      ],
      "metadata": {
        "id": "9xgeo42XpO7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike our previous image search, we'll need to encode our query Document this time (since last time our query Document had already been encoded)."
      ],
      "metadata": {
        "id": "IP4QgUR1ttNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_query = c.encode(text_query)"
      ],
      "metadata": {
        "id": "2D8taZ1Rtvhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_matches = docs.find(text_query)"
      ],
      "metadata": {
        "id": "8yInayemqyPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_matches[0].plot_image_sprites()"
      ],
      "metadata": {
        "id": "SH4XnhGPq75l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for a small dataset and pre-trained (un-finetuned) model!"
      ],
      "metadata": {
        "id": "75daLDQnqRjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéÅ Wrapping up\n",
        "\n",
        "Great - we've built a simple search engine in our notebook, with very few lines of code. But what's next?\n",
        "\n",
        "In future notebooks we'll see how you can:\n",
        "\n",
        "- [Finetune](https://finetuner.jina.ai?utm_source=notebook-ecommerce-1) the CLIP model for better results on your dataset.\n",
        "- Store your data on disk, spin up replicas/shards, start serving your search engine via a RESTful or gRPC gateway.\n",
        "- Use pre-existing [building blocks](https://hub.jina.ai?utm_source=notebook-ecommerce-1) to speed up development.\n",
        "- Host and deploy your search engine on [Jina Cloud](https://docs.jina.ai/fundamentals/jcloud?utm_source=notebook-ecommerce-1).\n",
        "\n",
        "Stay tuned to [the Jina AI blog](https://medium.com/jina-ai) or join our [Slack community](https://slack.jina.ai?utm_source=notebook-ecommerce-1) to keep up-to-date."
      ],
      "metadata": {
        "id": "CgYJeDaZi2Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Learn more\n",
        "\n",
        "Want to dig more into the Jina ecosystem? Here are some resources:\n",
        "\n",
        "- [Developer portal](https://learn.jina.ai?utm_source=notebook-ecommerce-1) - tutorials, courses, videos on using Jina\n",
        "- [Fashion search notebook](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb) - build an image-to-image fashion search engine\n",
        "- [DALL-E Flow](https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb)/[Disco Art](https://colab.research.google.com/github/jina-ai/discoart/blob/main/discoart.ipynb) - create AI-generated art in your browser"
      ],
      "metadata": {
        "id": "VkCMAAqsDcuC"
      }
    }
  ]
}
